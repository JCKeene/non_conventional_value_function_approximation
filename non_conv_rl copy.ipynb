{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from function_approximators.replay import ReplayBuffer\n",
    "import torch\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn import tree\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# env = WindyGridworldEnv()\n",
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def act(env, model, s, epsilon, explore):\n",
    "    if explore and np.random.random_sample() < epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else:       \n",
    "        try:\n",
    "            Q = [model.predict(np.concatenate([s, actions[i]],-1).reshape(1,-1)) for i in range(env.action_space.n)]\n",
    "            # print(Q)\n",
    "            action = np.argmax(Q)\n",
    "            # print(action)\n",
    "        except:\n",
    "            # print(\"init\")\n",
    "            action = env.action_space.sample()\n",
    "    return action\n",
    "\n",
    "def update(env, model, batch, gamma):\n",
    "    inputs = np.concatenate([batch.states, [actions[int(i.item())] for i in batch.actions]], -1)\n",
    "    preds = []\n",
    "    try:\n",
    "        for i in range(env.action_space.n):\n",
    "            next_inputs = np.concatenate([batch.next_states, np.zeros((batch.actions.size()[0], 1)) + actions[i]], -1)\n",
    "            preds.append(model.predict(next_inputs))\n",
    "        preds = np.array(preds).T\n",
    "        outputs = np.array(batch.rewards + gamma * (1-batch.done) * np.max(preds, 1).reshape(-1,1)).reshape(-1)\n",
    "    except:\n",
    "        # print(\"init\")\n",
    "        outputs = np.array(batch.rewards).reshape(-1)\n",
    "        \n",
    "    model.fit(inputs, outputs)\n",
    "    \n",
    "    # return q_loss\n",
    "\n",
    "\n",
    "def play_episode(env, model, replay_buffer, batch_size, gamma, epsilon, explore, train, episode_length):\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "    episode_timesteps = 0\n",
    "    episode_return = 0\n",
    "\n",
    "    while not done:\n",
    "        a = act(env, model, s, epsilon, explore=explore)\n",
    "        s_next, r, done, _ = env.step(a)\n",
    "        if train:\n",
    "            replay_buffer.push(\n",
    "                np.array(s, dtype=np.float32),\n",
    "                np.array([a], dtype=np.float32),\n",
    "                np.array(s_next, dtype=np.float32),\n",
    "                np.array([r], dtype=np.float32),\n",
    "                np.array([done], dtype=np.float32),\n",
    "                )\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                batch = replay_buffer.sample(batch_size)\n",
    "                update(env, model, batch, gamma)\n",
    "        episode_timesteps += 1\n",
    "        episode_return += r\n",
    "        \n",
    "        if episode_timesteps == episode_length:\n",
    "            break\n",
    "        s = s_next\n",
    "\n",
    "    return episode_timesteps, episode_return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeRegressor(max_depth=5, min_samples_split=5, min_samples_leaf=5)\n",
    "# model = RandomForestRegressor(n_estimators=5, max_depth=20)\n",
    "# model = MLPRegressor()\n",
    "replay_buffer = ReplayBuffer(1000)\n",
    "# actions = [[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]]\n",
    "actions =  [[1,0],[0,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  6%|▌         | 1106/20000 [00:00<00:15, 1207.07it/s]Evaluation at timestep 1010 returned a mean returns of 9.0\n",
      "Epsilon = 0.7\n",
      " 10%|█         | 2086/20000 [00:02<00:22, 781.77it/s]Evaluation at timestep 2007 returned a mean returns of 8.8\n",
      "Epsilon = 0.48999999999999994\n",
      " 16%|█▌        | 3134/20000 [00:03<00:23, 723.25it/s]Evaluation at timestep 3009 returned a mean returns of 12.6\n",
      "Epsilon = 0.3429999999999999\n",
      " 20%|██        | 4090/20000 [00:04<00:22, 719.46it/s]Evaluation at timestep 4000 returned a mean returns of 12.600000000000001\n",
      "Epsilon = 0.24009999999999992\n",
      " 25%|██▌       | 5075/20000 [00:06<00:25, 592.62it/s]Evaluation at timestep 5017 returned a mean returns of 179.4\n",
      "Epsilon = 0.16806999999999994\n",
      " 31%|███       | 6112/20000 [00:07<00:19, 710.17it/s]Evaluation at timestep 6025 returned a mean returns of 15.4\n",
      "Epsilon = 0.11764899999999995\n",
      " 35%|███▌      | 7097/20000 [00:09<00:18, 706.95it/s]Evaluation at timestep 7012 returned a mean returns of 9.8\n",
      "Epsilon = 0.08235429999999996\n",
      " 40%|████      | 8075/20000 [00:10<00:19, 618.88it/s]Evaluation at timestep 8001 returned a mean returns of 106.4\n",
      "Epsilon = 0.05764800999999997\n",
      " 45%|████▌     | 9082/20000 [00:12<00:18, 606.40it/s]Evaluation at timestep 9004 returned a mean returns of 158.8\n",
      "Epsilon = 0.04035360699999998\n",
      " 51%|█████     | 10150/20000 [00:13<00:14, 695.49it/s]Evaluation at timestep 10016 returned a mean returns of 17.6\n",
      "Epsilon = 0.03\n",
      " 56%|█████▌    | 11128/20000 [00:14<00:12, 706.86it/s]Evaluation at timestep 11004 returned a mean returns of 14.200000000000003\n",
      "Epsilon = 0.03\n",
      " 61%|██████    | 12126/20000 [00:16<00:11, 708.51it/s]Evaluation at timestep 12004 returned a mean returns of 11.200000000000001\n",
      "Epsilon = 0.03\n",
      " 66%|██████▌   | 13152/20000 [00:17<00:09, 708.02it/s]Evaluation at timestep 13016 returned a mean returns of 22.400000000000002\n",
      "Epsilon = 0.03\n",
      " 71%|███████   | 14107/20000 [00:19<00:08, 655.81it/s]Evaluation at timestep 14031 returned a mean returns of 91.8\n",
      "Epsilon = 0.03\n",
      " 76%|███████▌  | 15143/20000 [00:20<00:06, 702.17it/s]Evaluation at timestep 15022 returned a mean returns of 10.600000000000001\n",
      "Epsilon = 0.03\n",
      " 81%|████████  | 16150/20000 [00:22<00:06, 629.89it/s]Evaluation at timestep 16017 returned a mean returns of 171.4\n",
      "Epsilon = 0.03\n",
      " 86%|████████▌ | 17110/20000 [00:23<00:04, 706.32it/s]Evaluation at timestep 17009 returned a mean returns of 11.2\n",
      "Epsilon = 0.03\n",
      " 90%|█████████ | 18096/20000 [00:24<00:03, 602.63it/s]Evaluation at timestep 18010 returned a mean returns of 200.0\n",
      "Epsilon = 0.03\n",
      " 96%|█████████▌| 19150/20000 [00:26<00:01, 697.72it/s]Evaluation at timestep 19014 returned a mean returns of 19.0\n",
      "Epsilon = 0.03\n",
      "20010it [00:27, 727.05it/s]Evaluation at timestep 20010 returned a mean returns of 9.2\n",
      "Epsilon = 0.03\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_timesteps = 20000\n",
    "timesteps_elapsed = 0\n",
    "episode_length = 200\n",
    "eval_freq = 1000\n",
    "eval_episodes = 5\n",
    "gamma = 0.99\n",
    "epsilon = 1\n",
    "batch_size = 512\n",
    "\n",
    "with tqdm(total=max_timesteps) as pbar:\n",
    "\n",
    "    while timesteps_elapsed < max_timesteps:\n",
    "        episode_timesteps, _ = play_episode(env, model, replay_buffer, batch_size=batch_size, gamma=gamma, epsilon=epsilon, \n",
    "                                            explore=True, train=True, episode_length=episode_length)\n",
    "        timesteps_elapsed += episode_timesteps\n",
    "        pbar.update(episode_timesteps)\n",
    "\n",
    "        if timesteps_elapsed % eval_freq < episode_timesteps:\n",
    "            eval_returns = 0\n",
    "            for _ in range(eval_episodes):\n",
    "                _ , episode_return = play_episode(env, model, replay_buffer, batch_size=batch_size, gamma=gamma, epsilon=epsilon, \n",
    "                                                explore=False, train=False, episode_length=episode_length)\n",
    "                eval_returns += episode_return / eval_episodes\n",
    "\n",
    "            epsilon = max(epsilon*0.7,0.03)\n",
    "            # tree.plot_tree(model)\n",
    "            pbar.write(f\"Evaluation at timestep {timesteps_elapsed} returned a mean returns of {eval_returns}\")\n",
    "            pbar.write(f\"Epsilon = {epsilon}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[-0.08203987 -0.5493154   0.04367298  0.85068697  1.         -0.09302618\n -0.35481527  0.06068672  0.57205075]\n[-0.10012248 -0.5507333   0.07212774  0.8832178   1.         -0.11113715\n -0.3566611   0.08979209  0.61405355]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "import scipy.spatial.distance as dist\n",
    "n = replay_buffer.memory.states.shape[0]\n",
    "\n",
    "a = np.concatenate((replay_buffer.memory[0][0], replay_buffer.memory[1][0], replay_buffer.memory[2][0]))\n",
    "print(a)\n",
    "\n",
    "distances = []\n",
    "for i in range(n):\n",
    "    b = np.concatenate((replay_buffer.memory[0][i], replay_buffer.memory[1][i], replay_buffer.memory[2][i]))\n",
    "    distances.append(dist.euclidean(a,b))\n",
    "\n",
    "[(i,d) for (i,d) in enumerate(distances) if d < 0.1]\n",
    "\n",
    "print(np.concatenate((replay_buffer.memory[0][2], replay_buffer.memory[1][2], replay_buffer.memory[2][2])))\n",
    "\n",
    "min(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "low >= high",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c481018d60cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# model1 = DecisionTreeRegressor()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbatch1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mbatch1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_vfa/function_approximators/replay.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, batch_size, device)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         batch = Transition(\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.randint\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_bounded_integers.pyx\u001b[0m in \u001b[0;36mnumpy.random._bounded_integers._rand_int64\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: low >= high"
     ]
    }
   ],
   "source": [
    "# model1 = DecisionTreeRegressor()\n",
    "\n",
    "batch1 = replay_buffer.sample(32)\n",
    "batch1.rewards[3] += 1\n",
    "\n",
    "# inputs1 = np.concatenate([batch1.states, [actions[int(i.item())] for i in batch1.actions]], -1)\n",
    "# outputs1 = batch1.rewards\n",
    "\n",
    "inputs1 = np.concatenate([batch1.states, [actions[int(i.item())] for i in batch1.actions]], -1)\n",
    "preds1 =[]\n",
    "for i in range(env.action_space.n):\n",
    "    next_inputs1 = np.concatenate([batch1.next_states, np.zeros((batch1.actions.size()[0], 1))+actions[i]], -1)\n",
    "    preds1.append(model1.predict(next_inputs1))\n",
    "preds1 = np.array(preds1).T\n",
    "outputs1 = batch1.rewards + gamma * (1-batch1.done) * np.max(preds1, 1).reshape(-1,1)\n",
    "print(outputs1)\n",
    "tree.plot_tree(model1)\n",
    "\n",
    "model1.fit(inputs1, outputs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = replay_buffer.sample(32)\n",
    "np.concatenate([batch.next_states, np.zeros((batch.actions.size()[0], 1)) + actions[0]], -1).shape\n",
    "\n",
    "preds = []\n",
    "for i in range(env.action_space.n):\n",
    "    next_inputs = np.concatenate([batch.next_states, np.zeros((batch.actions.size()[0], 1)) + actions[i]], -1)\n",
    "    preds.append(model.predict(next_inputs))\n",
    "preds = np.array(preds).T\n",
    "outputs = np.array(batch.rewards + gamma * (1-batch.done) * np.max(preds, 1).reshape(-1,1)).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(32,)"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(32,)"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "np.array(batch.rewards).reshape(-1).shape"
   ]
  }
 ]
}