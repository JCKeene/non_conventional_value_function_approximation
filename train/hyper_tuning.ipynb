{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd022a4fa19595f02a13987d52fdee973c5ab514be50202bb6cec211f6ae6538c08",
   "display_name": "Python 3.8.8 64-bit ('rl_vfa_venv': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import numpy as np\n",
    "from custom_envs.windy_gridworld import WindyGridworldEnv\n",
    "\n",
    "from function_approximators.function_approximators import NeuralNetwork, LinearModel\n",
    "from train_utils import train\n",
    "\n",
    "from agents.agents import DQNAgent, LinearAgent\n",
    "\n",
    "RENDER = False\n",
    "# env = gym.make(\"CartPole-v1\")\n",
    "# env = gym.make(\"MountainCar-v0\")\n",
    "env = WindyGridworldEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_approximators = [NeuralNetwork, LinearModel]\n",
    "agents = [DQNAgent, LinearAgent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  1%|          | 200/20000 [00:00<00:12, 1549.30it/s]\n",
      " Run: 1 \n",
      "\n",
      "q_loss: 0.818901538848877\n",
      "  6%|▌         | 1200/20000 [00:01<00:19, 975.20it/s] Evaluation at timestep 1000 returned a mean returns of -200.0\n",
      "Epsilon = 0.9224\n",
      "Learning rate = 0.0005\n",
      "q_loss: 0.8332507610321045\n",
      " 11%|█         | 2200/20000 [00:02<00:20, 888.45it/s] Evaluation at timestep 2000 returned a mean returns of -200.0\n",
      "Epsilon = 0.8254\n",
      "Learning rate = 0.000495\n",
      "q_loss: 0.8069630861282349\n",
      " 16%|█▌        | 3200/20000 [00:02<00:16, 1035.50it/s]Evaluation at timestep 3000 returned a mean returns of -200.0\n",
      "Epsilon = 0.7283999999999999\n",
      "Learning rate = 0.00049005\n",
      "q_loss: 0.8369412422180176\n",
      " 20%|██        | 4069/20000 [00:03<00:11, 1350.63it/s]q_loss: 0.8672789931297302\n",
      "Evaluation at timestep 4069 returned a mean returns of -200.0\n",
      "Epsilon = 0.624707\n",
      "Learning rate = 0.00048029800499999997\n",
      " 25%|██▌       | 5069/20000 [00:04<00:11, 1292.08it/s]q_loss: 0.811018705368042\n",
      "Evaluation at timestep 5069 returned a mean returns of -200.0\n",
      "Epsilon = 0.527707\n",
      "Learning rate = 0.00047549502494999995\n",
      " 30%|███       | 6069/20000 [00:05<00:10, 1323.92it/s]q_loss: 0.8097991347312927\n",
      "Evaluation at timestep 6069 returned a mean returns of -200.0\n",
      "Epsilon = 0.43070700000000006\n",
      "Learning rate = 0.00047074007470049995\n",
      " 35%|███▌      | 7069/20000 [00:06<00:10, 1206.00it/s]q_loss: 0.827221155166626\n",
      "Evaluation at timestep 7069 returned a mean returns of -200.0\n",
      "Epsilon = 0.3337070000000001\n",
      "Learning rate = 0.00046603267395349497\n",
      " 40%|████      | 8069/20000 [00:07<00:09, 1271.07it/s]q_loss: 0.9926774501800537\n",
      "Evaluation at timestep 8069 returned a mean returns of -200.0\n",
      "Epsilon = 0.236707\n",
      "Learning rate = 0.00046137234721396\n",
      " 45%|████▌     | 9069/20000 [00:08<00:08, 1276.47it/s]q_loss: 0.8467152118682861\n",
      "Evaluation at timestep 9069 returned a mean returns of -200.0\n",
      "Epsilon = 0.13970700000000003\n",
      "Learning rate = 0.0004567586237418204\n",
      " 50%|█████     | 10069/20000 [00:08<00:08, 1170.82it/s]q_loss: 0.9508504867553711\n",
      "Evaluation at timestep 10069 returned a mean returns of -200.0\n",
      "Epsilon = 0.04270700000000005\n",
      "Learning rate = 0.00045219103750440215\n",
      " 55%|█████▌    | 11069/20000 [00:10<00:08, 1066.93it/s]q_loss: 0.7836479544639587\n",
      "Evaluation at timestep 11069 returned a mean returns of -200.0\n",
      "Epsilon = 0.030000000000000027\n",
      "Learning rate = 0.00044766912712935813\n",
      " 60%|██████    | 12069/20000 [00:10<00:06, 1198.09it/s]q_loss: 0.7318472862243652\n",
      "Evaluation at timestep 12069 returned a mean returns of -200.0\n",
      "Epsilon = 0.030000000000000027\n",
      "Learning rate = 0.00044319243585806456\n",
      " 65%|██████▌   | 13069/20000 [00:11<00:05, 1224.49it/s]q_loss: 1.022596836090088\n",
      "Evaluation at timestep 13069 returned a mean returns of -200.0\n",
      "Epsilon = 0.030000000000000027\n",
      "Learning rate = 0.0004387605114994839\n",
      " 70%|███████   | 14069/20000 [00:12<00:04, 1256.95it/s]q_loss: 1.0163191556930542\n",
      "Evaluation at timestep 14069 returned a mean returns of -200.0\n",
      "Epsilon = 0.030000000000000027\n",
      "Learning rate = 0.00043437290638448904\n",
      " 76%|███████▌  | 15190/20000 [00:13<00:03, 1282.08it/s]q_loss: 1.0437067747116089\n",
      " 77%|███████▋  | 15390/20000 [00:13<00:04, 1044.72it/s]Evaluation at timestep 15190 returned a mean returns of -200.0\n",
      "Epsilon = 0.030000000000000027\n",
      "Learning rate = 0.00043002917732064413\n",
      " 81%|████████  | 16190/20000 [00:14<00:03, 1207.29it/s]q_loss: 0.997814416885376\n",
      " 82%|████████▏ | 16390/20000 [00:14<00:03, 986.44it/s] Evaluation at timestep 16190 returned a mean returns of -200.0\n",
      "Epsilon = 0.030000000000000027\n",
      "Learning rate = 0.0004257288855474377\n",
      " 86%|████████▌ | 17190/20000 [00:15<00:02, 1209.91it/s]q_loss: 0.7904311418533325\n",
      " 87%|████████▋ | 17390/20000 [00:15<00:02, 995.71it/s] Evaluation at timestep 17190 returned a mean returns of -200.0\n",
      "Epsilon = 0.030000000000000027\n",
      "Learning rate = 0.0004214715966919633\n",
      " 91%|█████████ | 18190/20000 [00:16<00:01, 1226.27it/s]q_loss: 0.5540024042129517\n",
      " 91%|█████████ | 18190/20000 [00:16<00:01, 1226.27it/s]Evaluation at timestep 18190 returned a mean returns of -200.0\n",
      "Epsilon = 0.030000000000000027\n",
      "Learning rate = 0.00041725688072504364\n",
      " 95%|█████████▌| 19071/20000 [00:17<00:00, 1186.41it/s]q_loss: 1.7053805589675903\n",
      "Evaluation at timestep 19071 returned a mean returns of -200.0\n",
      "Epsilon = 0.030000000000000027\n",
      "Learning rate = 0.0004130843119177932\n",
      "20071it [00:18, 1112.47it/s]\n",
      "  0%|          | 0/20000 [00:00<?, ?it/s]q_loss: 0.950239896774292\n",
      "Evaluation at timestep 20071 returned a mean returns of -200.0\n",
      "Epsilon = 0.030000000000000027\n",
      "Learning rate = 0.0004089534687986153\n",
      "\n",
      " Run: 2 \n",
      "\n",
      "q_loss: 0.7307430505752563\n",
      "  6%|▌         | 1200/20000 [00:00<00:17, 1072.06it/s]Evaluation at timestep 1000 returned a mean returns of -200.0\n",
      "Epsilon = 0.9224\n",
      "Learning rate = 0.0005\n",
      "q_loss: 0.7569888830184937\n",
      " 10%|█         | 2000/20000 [00:01<00:14, 1239.65it/s]Evaluation at timestep 2000 returned a mean returns of -200.0\n",
      "Epsilon = 0.8254\n",
      "Learning rate = 0.000495\n",
      "q_loss: 0.8958080410957336\n",
      " 16%|█▌        | 3200/20000 [00:02<00:18, 912.68it/s] Evaluation at timestep 3000 returned a mean returns of -200.0\n",
      "Epsilon = 0.7283999999999999\n",
      "Learning rate = 0.00049005\n",
      "q_loss: 0.9250268936157227\n",
      " 17%|█▋        | 3400/20000 [00:03<00:15, 1076.18it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-9334b0d4850f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;34m\"poly_degree\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     }\n\u001b[0;32m---> 34\u001b[0;31m     r, _ = train(env, \n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0mCONFIG\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mfa\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction_approximators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_vfa/train/train_utils.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(env, config, fa, agent, output, render)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule_hyperparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimesteps_elapsed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"max_timesteps\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             episode_timesteps, _, losses = play_episode(\n\u001b[0m\u001b[1;32m     86\u001b[0m                 \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_vfa/train/train_utils.py\u001b[0m in \u001b[0;36mplay_episode\u001b[0;34m(env, agent, replay_buffer, train, explore, render, max_steps, batch_size)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mnext_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             replay_buffer.push(\n\u001b[0m\u001b[1;32m     34\u001b[0m                 \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_vfa/function_approximators/replay.py\u001b[0m in \u001b[0;36mpush\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mtransition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtransition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mtransition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m# initialise replay buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# for lr in np.linspace(0.0005,0.005,10):\n",
    "\n",
    "returns = []\n",
    "n_seeds=10\n",
    "\n",
    "for i in range(n_seeds):\n",
    "\n",
    "    print(f\"\\n Run: {i+1} \\n\")\n",
    "\n",
    "    # print(f\"Learning rate: {lr}\")\n",
    "    # Linear Config\n",
    "    # DQN Config\n",
    "    CONFIG = {\n",
    "        \"episode_length\": 200,\n",
    "        \"max_timesteps\": 20000,\n",
    "        \"max_time\": 30 * 60,\n",
    "        \"eval_freq\": 1000, \n",
    "        \"eval_episodes\": 10,\n",
    "        \"learning_rate\": 0.005,\n",
    "        # \"hidden_size\": (16,16),\n",
    "        \"target_update_freq\": 200,\n",
    "        \"batch_size\": 32,\n",
    "        \"gamma\": 0.99,\n",
    "        \"buffer_capacity\": int(1e7),\n",
    "        \"plot_loss\": False,\n",
    "        \"epsilon\": 1,\n",
    "        \"max_deduct\": 0.95,\n",
    "        \"decay\": 0.5,\n",
    "        \"lr_step_size\": 1000,\n",
    "        \"lr_gamma\": 0.99,\n",
    "        \"max_steps\": 200,\n",
    "        \"poly_degree\": 1,\n",
    "    }\n",
    "    r, _ = train(env, \n",
    "            CONFIG, \n",
    "            fa=function_approximators[1], \n",
    "            agent = agents[1], \n",
    "            render=RENDER)\n",
    "    env.close()\n",
    "    returns.append(r[-1])\n",
    "\n",
    "print(f\"Mean returns: {np.mean(returns)}\")\n",
    "print(f\"Std returns: {np.std(returns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}