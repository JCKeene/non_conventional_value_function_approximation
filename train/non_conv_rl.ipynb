{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd022a4fa19595f02a13987d52fdee973c5ab514be50202bb6cec211f6ae6538c08",
   "display_name": "Python 3.8.8 64-bit ('rl_vfa_venv': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "from custom_envs.windy_gridworld import WindyGridworldEnv\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from function_approximators.replay import ReplayBuffer\n",
    "import torch\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn import tree\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# env = WindyGridworldEnv()\n",
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def act(env, model, s, epsilon, explore):\n",
    "    if explore and np.random.random_sample() < epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else:       \n",
    "        try:\n",
    "            Q = [model.predict(np.concatenate([s, actions[i]],-1).reshape(1,-1)) for i in range(env.action_space.n)]\n",
    "            # print(Q)\n",
    "            action = np.argmax(Q)\n",
    "            # print(action)\n",
    "        except:\n",
    "            # print(\"init\")\n",
    "            action = env.action_space.sample()\n",
    "    return action\n",
    "\n",
    "def update(env, model, batch, gamma):\n",
    "    inputs = np.concatenate([batch.states, [actions[int(i.item())] for i in batch.actions]], -1)\n",
    "    preds = []\n",
    "    try:\n",
    "        for i in range(env.action_space.n):\n",
    "            next_inputs = np.concatenate([batch.next_states, np.zeros((batch.actions.size()[0], 1)) + actions[i]], -1)\n",
    "            preds.append(model.predict(next_inputs))\n",
    "        preds = np.array(preds).T\n",
    "        outputs = np.array(batch.rewards + gamma * (1-batch.done) * np.max(preds, 1).reshape(-1,1)).reshape(-1)\n",
    "    except:\n",
    "        # print(\"init\")\n",
    "        outputs = np.array(batch.rewards).reshape(-1)\n",
    "        \n",
    "    model.fit(inputs, outputs)\n",
    "    \n",
    "    # return q_loss\n",
    "\n",
    "\n",
    "def play_episode(env, model, replay_buffer, batch_size, gamma, epsilon, explore, train, episode_length):\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "    episode_timesteps = 0\n",
    "    episode_return = 0\n",
    "\n",
    "    while not done:\n",
    "        a = act(env, model, s, epsilon, explore=explore)\n",
    "        s_next, r, done, _ = env.step(a)\n",
    "        if train:\n",
    "            replay_buffer.push(\n",
    "                np.array(s, dtype=np.float32),\n",
    "                np.array([a], dtype=np.float32),\n",
    "                np.array(s_next, dtype=np.float32),\n",
    "                np.array([r], dtype=np.float32),\n",
    "                np.array([done], dtype=np.float32),\n",
    "                )\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                batch = replay_buffer.sample(batch_size)\n",
    "                update(env, model, batch, gamma)\n",
    "        episode_timesteps += 1\n",
    "        episode_return += r\n",
    "        \n",
    "        if episode_timesteps == episode_length:\n",
    "            break\n",
    "        s = s_next\n",
    "\n",
    "    return episode_timesteps, episode_return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = DecisionTreeRegressor(max_depth=30)\n",
    "model = RandomForestRegressor(n_estimators=5, max_depth=20)\n",
    "# model = MLPRegressor()\n",
    "replay_buffer = ReplayBuffer(100000)\n",
    "# actions = [[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]]\n",
    "actions =  [[1,0],[0,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  5%|▌         | 1009/20000 [00:00<00:00, 23066.73it/s]Evaluation at timestep 1009 returned a mean returns of 27.0\n",
      "Epsilon = 0.7\n",
      " 10%|█         | 2010/20000 [00:22<06:23, 46.94it/s]Evaluation at timestep 2010 returned a mean returns of 124.19999999999999\n",
      "Epsilon = 0.48999999999999994\n",
      " 15%|█▌        | 3001/20000 [00:45<06:25, 44.04it/s]Evaluation at timestep 3001 returned a mean returns of 142.60000000000002\n",
      "Epsilon = 0.3429999999999999\n",
      " 20%|██        | 4070/20000 [01:11<06:09, 43.08it/s]Evaluation at timestep 4070 returned a mean returns of 182.0\n",
      "Epsilon = 0.24009999999999992\n",
      " 25%|██▌       | 5025/20000 [01:33<05:40, 43.97it/s]Evaluation at timestep 5025 returned a mean returns of 134.0\n",
      "Epsilon = 0.16806999999999994\n",
      " 31%|███       | 6128/20000 [01:59<05:12, 44.45it/s]Evaluation at timestep 6128 returned a mean returns of 181.20000000000002\n",
      "Epsilon = 0.11764899999999995\n",
      " 35%|███▌      | 7038/20000 [02:19<04:47, 45.04it/s]Evaluation at timestep 7038 returned a mean returns of 140.79999999999998\n",
      "Epsilon = 0.08235429999999996\n",
      " 40%|████      | 8099/20000 [02:43<04:23, 45.11it/s]Evaluation at timestep 8099 returned a mean returns of 101.80000000000001\n",
      "Epsilon = 0.05764800999999997\n",
      " 45%|████▌     | 9057/20000 [03:05<04:03, 44.98it/s]Evaluation at timestep 9057 returned a mean returns of 153.8\n",
      "Epsilon = 0.05\n",
      " 51%|█████     | 10116/20000 [03:28<03:30, 46.93it/s]Evaluation at timestep 10116 returned a mean returns of 33.8\n",
      "Epsilon = 0.05\n",
      " 56%|█████▌    | 11163/20000 [03:51<03:12, 45.94it/s]Evaluation at timestep 11163 returned a mean returns of 123.8\n",
      "Epsilon = 0.05\n",
      " 60%|██████    | 12021/20000 [04:11<03:02, 43.83it/s]Evaluation at timestep 12021 returned a mean returns of 68.80000000000001\n",
      "Epsilon = 0.05\n",
      " 65%|██████▌   | 13000/20000 [04:33<02:34, 45.23it/s]Evaluation at timestep 13000 returned a mean returns of 97.20000000000002\n",
      "Epsilon = 0.05\n",
      " 70%|███████   | 14012/20000 [04:57<02:31, 39.62it/s]Evaluation at timestep 14012 returned a mean returns of 102.20000000000002\n",
      "Epsilon = 0.05\n",
      " 75%|███████▌  | 15013/20000 [05:22<02:02, 40.76it/s]Evaluation at timestep 15013 returned a mean returns of 93.0\n",
      "Epsilon = 0.05\n",
      " 80%|████████  | 16009/20000 [05:48<01:39, 40.26it/s]Evaluation at timestep 16009 returned a mean returns of 107.4\n",
      "Epsilon = 0.05\n",
      " 85%|████████▌ | 17006/20000 [06:14<01:14, 40.10it/s]Evaluation at timestep 17006 returned a mean returns of 87.19999999999999\n",
      "Epsilon = 0.05\n",
      " 90%|█████████ | 18083/20000 [06:41<00:46, 41.19it/s]Evaluation at timestep 18083 returned a mean returns of 146.20000000000002\n",
      "Epsilon = 0.05\n",
      " 95%|█████████▌| 19094/20000 [07:06<00:22, 41.13it/s]Evaluation at timestep 19094 returned a mean returns of 57.0\n",
      "Epsilon = 0.05\n",
      "20007it [07:28, 44.64it/s]Evaluation at timestep 20007 returned a mean returns of 68.0\n",
      "Epsilon = 0.05\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_timesteps = 20000\n",
    "timesteps_elapsed = 0\n",
    "episode_length = 200\n",
    "eval_freq = 1000\n",
    "eval_episodes = 5\n",
    "gamma = 0.99\n",
    "epsilon = 1\n",
    "batch_size = 1024\n",
    "\n",
    "with tqdm(total=max_timesteps) as pbar:\n",
    "\n",
    "    while timesteps_elapsed < max_timesteps:\n",
    "        episode_timesteps, _ = play_episode(env, model, replay_buffer, batch_size=batch_size, gamma=gamma, epsilon=epsilon, \n",
    "                                            explore=True, train=True, episode_length=episode_length)\n",
    "        timesteps_elapsed += episode_timesteps\n",
    "        pbar.update(episode_timesteps)\n",
    "\n",
    "        if timesteps_elapsed % eval_freq < episode_timesteps:\n",
    "            eval_returns = 0\n",
    "            for _ in range(eval_episodes):\n",
    "                _ , episode_return = play_episode(env, model, replay_buffer, batch_size=batch_size, gamma=gamma, epsilon=epsilon, \n",
    "                                                explore=False, train=False, episode_length=episode_length)\n",
    "                eval_returns += episode_return / eval_episodes\n",
    "\n",
    "            epsilon = max(epsilon*0.7,0.05)\n",
    "            # tree.plot_tree(model)\n",
    "            pbar.write(f\"Evaluation at timestep {timesteps_elapsed} returned a mean returns of {eval_returns}\")\n",
    "            pbar.write(f\"Epsilon = {epsilon}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "low >= high",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c481018d60cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# model1 = DecisionTreeRegressor()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbatch1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mbatch1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_vfa/function_approximators/replay.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, batch_size, device)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         batch = Transition(\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.randint\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_bounded_integers.pyx\u001b[0m in \u001b[0;36mnumpy.random._bounded_integers._rand_int64\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: low >= high"
     ]
    }
   ],
   "source": [
    "# model1 = DecisionTreeRegressor()\n",
    "\n",
    "batch1 = replay_buffer.sample(32)\n",
    "batch1.rewards[3] += 1\n",
    "\n",
    "# inputs1 = np.concatenate([batch1.states, [actions[int(i.item())] for i in batch1.actions]], -1)\n",
    "# outputs1 = batch1.rewards\n",
    "\n",
    "inputs1 = np.concatenate([batch1.states, [actions[int(i.item())] for i in batch1.actions]], -1)\n",
    "preds1 =[]\n",
    "for i in range(env.action_space.n):\n",
    "    next_inputs1 = np.concatenate([batch1.next_states, np.zeros((batch1.actions.size()[0], 1))+actions[i]], -1)\n",
    "    preds1.append(model1.predict(next_inputs1))\n",
    "preds1 = np.array(preds1).T\n",
    "outputs1 = batch1.rewards + gamma * (1-batch1.done) * np.max(preds1, 1).reshape(-1,1)\n",
    "print(outputs1)\n",
    "tree.plot_tree(model1)\n",
    "\n",
    "model1.fit(inputs1, outputs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = replay_buffer.sample(32)\n",
    "np.concatenate([batch.next_states, np.zeros((batch.actions.size()[0], 1)) + actions[0]], -1).shape\n",
    "\n",
    "preds = []\n",
    "for i in range(env.action_space.n):\n",
    "    next_inputs = np.concatenate([batch.next_states, np.zeros((batch.actions.size()[0], 1)) + actions[i]], -1)\n",
    "    preds.append(model.predict(next_inputs))\n",
    "preds = np.array(preds).T\n",
    "outputs = np.array(batch.rewards + gamma * (1-batch.done) * np.max(preds, 1).reshape(-1,1)).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(32,)"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(32,)"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "np.array(batch.rewards).reshape(-1).shape"
   ]
  }
 ]
}