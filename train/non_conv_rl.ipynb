{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd022a4fa19595f02a13987d52fdee973c5ab514be50202bb6cec211f6ae6538c08",
   "display_name": "Python 3.8.8 64-bit ('rl_vfa_venv': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "from custom_envs.windy_gridworld import WindyGridworldEnv\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from function_approximators.replay import ReplayBuffer\n",
    "import torch\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "env = WindyGridworldEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def act(env, model, s, epsilon, explore):\n",
    "    if explore and np.random.random_sample() < epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else:       \n",
    "        Q = [model.predict(np.concatenate([s_next, [i]],-1).reshape(1,-1)) for i in range(4)]\n",
    "        action = np.argmax(Q)\n",
    "        action = env.action_space.sample()\n",
    "    return action\n",
    "\n",
    "def update(env, model, batch, gamma):\n",
    "    inputs = np.concatenate([batch.states, batch.actions], -1)\n",
    "    preds = []\n",
    "    try:\n",
    "        # check_is_fitted(model)\n",
    "        for i in range(env.action_space.n):\n",
    "            next_inputs = np.concatenate([batch.next_states, np.zeros((batch.actions.size()[0], 1))+i], -1)\n",
    "            preds.append(model.predict(next_inputs))\n",
    "        outputs = batch.rewards + gamma * (1-batch.done) * np.max(preds, 0)\n",
    "    except:\n",
    "        outputs = batch.rewards\n",
    "        \n",
    "    model.fit(inputs, outputs)\n",
    "\n",
    "    # return q_loss\n",
    "\n",
    "\n",
    "def play_episode(env, model, replay_buffer, batch_size, gamma, epsilon, explore, train, episode_length):\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "    episode_timesteps = 0\n",
    "    episode_return = 0\n",
    "\n",
    "    while not done:\n",
    "        a = act(env, model, s, epsilon, explore=explore)\n",
    "        s_next, r, done, _ = env.step(a)\n",
    "        if train:\n",
    "            replay_buffer.push(\n",
    "                np.array(s, dtype=np.float32),\n",
    "                np.array([a], dtype=np.float32),\n",
    "                np.array(s_next, dtype=np.float32),\n",
    "                np.array([r], dtype=np.float32),\n",
    "                np.array([done], dtype=np.float32),\n",
    "                )\n",
    "            if len(replay_buffer) >= batch_size:\n",
    "                batch = replay_buffer.sample(batch_size)\n",
    "                update(env, model, batch, gamma)\n",
    "        episode_timesteps += 1\n",
    "        episode_return += r\n",
    "        \n",
    "        if episode_timesteps == episode_length:\n",
    "            break\n",
    "        s = s_next\n",
    "\n",
    "    return episode_timesteps, episode_return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeRegressor()\n",
    "replay_buffer = ReplayBuffer(1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  6%|▌         | 1100/20000 [00:01<00:36, 523.29it/s]Evaluation at timestep 1000 returned a mean returns of -100.0\n",
      "Epsilon = 0.1\n",
      " 10%|█         | 2100/20000 [00:03<00:31, 576.28it/s]Evaluation at timestep 2000 returned a mean returns of -100.0\n",
      "Epsilon = 0.1\n",
      " 16%|█▌        | 3100/20000 [00:04<00:28, 595.35it/s]Evaluation at timestep 3000 returned a mean returns of -100.0\n",
      "Epsilon = 0.1\n",
      " 21%|██        | 4162/20000 [00:05<00:27, 579.77it/s]Evaluation at timestep 4062 returned a mean returns of -100.0\n",
      "Epsilon = 0.1\n",
      " 26%|██▌       | 5132/20000 [00:07<00:24, 609.04it/s]Evaluation at timestep 5032 returned a mean returns of -100.0\n",
      "Epsilon = 0.1\n",
      " 31%|███       | 6132/20000 [00:08<00:23, 590.12it/s]Evaluation at timestep 6032 returned a mean returns of -100.0\n",
      "Epsilon = 0.1\n",
      " 36%|███▌      | 7132/20000 [00:10<00:22, 564.28it/s]Evaluation at timestep 7032 returned a mean returns of -100.0\n",
      "Epsilon = 0.1\n",
      " 41%|████      | 8132/20000 [00:11<00:20, 590.96it/s]Evaluation at timestep 8032 returned a mean returns of -100.0\n",
      "Epsilon = 0.1\n",
      " 46%|████▌     | 9132/20000 [00:12<00:17, 605.11it/s]Evaluation at timestep 9032 returned a mean returns of -100.0\n",
      "Epsilon = 0.1\n",
      " 51%|█████     | 10132/20000 [00:14<00:16, 605.70it/s]Evaluation at timestep 10032 returned a mean returns of -100.0\n",
      "Epsilon = 0.1\n",
      " 56%|█████▌    | 11132/20000 [00:15<00:14, 614.10it/s]Evaluation at timestep 11032 returned a mean returns of -100.0\n",
      "Epsilon = 0.1\n",
      " 61%|██████    | 12132/20000 [00:16<00:14, 552.30it/s]Evaluation at timestep 12032 returned a mean returns of -100.0\n",
      "Epsilon = 0.1\n",
      " 66%|██████▌   | 13132/20000 [00:18<00:11, 607.05it/s]Evaluation at timestep 13032 returned a mean returns of -100.0\n",
      "Epsilon = 0.1\n",
      " 71%|███████   | 14132/20000 [00:19<00:11, 495.98it/s]Evaluation at timestep 14032 returned a mean returns of -100.0\n",
      "Epsilon = 0.1\n",
      " 76%|███████▌  | 15132/20000 [00:21<00:09, 536.99it/s]Evaluation at timestep 15032 returned a mean returns of -100.0\n",
      "Epsilon = 0.1\n",
      " 81%|████████  | 16132/20000 [00:22<00:07, 536.28it/s]Evaluation at timestep 16032 returned a mean returns of -100.0\n",
      "Epsilon = 0.1\n",
      " 86%|████████▌ | 17132/20000 [00:24<00:05, 504.33it/s]Evaluation at timestep 17032 returned a mean returns of -100.0\n",
      "Epsilon = 0.1\n",
      " 91%|█████████ | 18132/20000 [00:26<00:04, 449.18it/s]Evaluation at timestep 18032 returned a mean returns of -100.0\n",
      "Epsilon = 0.1\n",
      " 96%|█████████▌| 19132/20000 [00:28<00:01, 526.87it/s]Evaluation at timestep 19032 returned a mean returns of -100.0\n",
      "Epsilon = 0.1\n",
      "20032it [00:29, 682.62it/s]Evaluation at timestep 20032 returned a mean returns of -100.0\n",
      "Epsilon = 0.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_timesteps = 20000\n",
    "timesteps_elapsed = 0\n",
    "train = True\n",
    "episode_length = 100\n",
    "eval_freq = 1000\n",
    "eval_episodes = 5\n",
    "gamma = 0.99\n",
    "epsilon = 0.1\n",
    "batch_size = 128\n",
    "\n",
    "with tqdm(total=max_timesteps) as pbar:\n",
    "\n",
    "    while timesteps_elapsed < max_timesteps:\n",
    "        episode_timesteps, _ = play_episode(env, model, replay_buffer, batch_size=batch_size, gamma=gamma, epsilon=epsilon, \n",
    "                                            explore=True, train=True, episode_length=episode_length)\n",
    "        timesteps_elapsed += episode_timesteps\n",
    "        pbar.update(episode_timesteps)\n",
    "\n",
    "        if timesteps_elapsed % eval_freq < episode_timesteps:\n",
    "            eval_returns = 0\n",
    "            for _ in range(eval_episodes):\n",
    "                _ , episode_return = play_episode(env, model, replay_buffer, batch_size=batch_size, gamma=gamma, epsilon=epsilon, \n",
    "                                                explore=False, train=False, episode_length=episode_length)\n",
    "                eval_returns += episode_return / eval_episodes\n",
    "\n",
    "            pbar.write(f\"Evaluation at timestep {timesteps_elapsed} returned a mean returns of {eval_returns}\")\n",
    "            pbar.write(f\"Epsilon = {epsilon}\")\n",
    "            # pbar.write(f\"Learning rate = {agent.model_optim.param_groups[0]['lr']}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[-1.9900, -1.9900, -1.9900,  ..., -1.9900, -1.9900, -1.9900],\n",
       "        [-1.9900, -1.9900, -1.9900,  ..., -1.9900, -1.9900, -1.9900],\n",
       "        [-1.9900, -1.9900, -1.9900,  ..., -1.9900, -1.9900, -1.9900],\n",
       "        ...,\n",
       "        [-1.9900, -1.9900, -1.9900,  ..., -1.9900, -1.9900, -1.9900],\n",
       "        [-1.9900, -1.9900, -1.9900,  ..., -1.9900, -1.9900, -1.9900],\n",
       "        [-1.9900, -1.9900, -1.9900,  ..., -1.9900, -1.9900, -1.9900]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "metadata": {},
     "execution_count": 108
    }
   ],
   "source": [
    "batch = replay_buffer.sample(32)\n",
    "inputs = np.concatenate([batch.states, batch.actions], -1)\n",
    "outputs = batch.rewards\n",
    "model.fit(inputs, outputs)\n",
    "\n",
    "preds = []\n",
    "for i in range(4):\n",
    "    next_inputs = np.concatenate([batch.next_states, np.zeros((batch.actions.size()[0],1))+i], -1)\n",
    "    preds.append(model.predict(next_inputs))\n",
    "a = batch.rewards + gamma*np.max(preds, 0)\n",
    "a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "metadata": {},
     "execution_count": 155
    }
   ],
   "source": [
    "a=1\n",
    "s_next, r, done, _ = env.step(1)\n",
    "Q = [model.predict(np.concatenate([s_next, [i]],-1).reshape(1,-1)) for i in range(4)]\n",
    "np.argmax(Q)\n",
    "# model.predict(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([-1., -1., -1., -1.])"
      ]
     },
     "metadata": {},
     "execution_count": 149
    }
   ],
   "source": [
    "# batch = replay_buffer.sample(4)\n",
    "# inputs = np.concatenate([batch.states, batch.actions], -1)\n",
    "model.predict(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DecisionTreeRegressor()"
      ]
     },
     "metadata": {},
     "execution_count": 148
    }
   ],
   "source": [
    "model.fit(inputs, batch.rewards)"
   ]
  }
 ]
}